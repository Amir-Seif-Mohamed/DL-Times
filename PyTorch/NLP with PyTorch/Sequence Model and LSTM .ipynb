{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models and Long-Short Term Memory Networks\n",
    "----------------------------------------------------------\n",
    "At this point, we have seen various feed-forward networks. That is, there is no state maintained by the network at all. This might not be the behavior we want. Sequence models are central to NLP: they are models where there is some sort of dependence through time between your inputs. The classical example of a sequence model is the Hidden Markov Model for part-of-speech tagging. Another example is the conditional random field.\n",
    "\n",
    "A recurrent neural network is a network that maintains some kind of state. For example, its output could be used as part of the next input, so that information can propogate along as the network passes over the sequence. In the case of an LSTM, for each element in the sequence, there is a corresponding hidden state ht, which in principle can contain information from arbitrary points earlier in the sequence. We can use the hidden state to predict words in a language model, part-of-speech tags, and a myriad of other things.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff11c119610>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Hidden States :\n",
      "tensor([[[-0.0187,  0.1713, -0.2944]],\n",
      "\n",
      "        [[-0.3521,  0.1026, -0.2971]],\n",
      "\n",
      "        [[-0.3191,  0.0781, -0.1957]],\n",
      "\n",
      "        [[-0.1634,  0.0941, -0.1637]],\n",
      "\n",
      "        [[-0.3368,  0.0959, -0.0538]]])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The latest/last hidden state :\n",
      "(tensor([[[-0.3368,  0.0959, -0.0538]]]), tensor([[[-0.9825,  0.4715, -0.0633]]]))\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)\n",
    "\n",
    "# Making input of Seq 5, batch size of 1 each having 3 words\n",
    "inputs = [torch.randn(1,3) for _ in range(5)] \n",
    "\n",
    "# initialize the hidden layer\n",
    "# our of this one is the initial hidden state\n",
    "# and another is the initial cell state\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "\n",
    "for i in inputs:\n",
    "    # Step through the sequence onc element at a time\n",
    "    # after each step hidden contain the hidden state\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    \n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(\"All Hidden States :\\n{}\".format(out))\n",
    "print('-' * 100)\n",
    "print(\"The latest/last hidden state :\\n{}\".format(hidden))\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'newspaper': 8}\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(seq, to_ix):\n",
    "    idsx = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idsx, dtype = torch.long)\n",
    "\n",
    "training_data = [\n",
    "    ('The dog ate the apple'.split(), ['DET', 'NN', 'V', 'DET', 'NN']),\n",
    "    ('Everybody read that newspaper'.split(), ['NN', 'V', 'DET', 'NN'])\n",
    "]\n",
    "\n",
    "word_to_ix = {}\n",
    "\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "print(word_to_ix)\n",
    "\n",
    "tags_to_ix = {'DET' : 0, 'NN' : 1, 'V' : 2}\n",
    "ix_to_tags = {0 : 'DET', 1 : 'NN', 2 : 'V'}\n",
    "\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedd_dim, hidden_dim, vocab_size, tag_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedd_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as input and output hidden layer\n",
    "        # with hidden dimesion\n",
    "        self.lstm = nn.LSTM(embedd_dim, hidden_dim)\n",
    "        \n",
    "        # The linear layer that map the hidden layer to the tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tag_size)\n",
    "        # Initialise the initial state layer and cell state\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Initialising the initial hidden state and cell state\n",
    "        # The axes semanntics are (num_layers, mini_batch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim), \n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        \n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = f.log_softmax(tag_space, dim = 1)\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(embedd_dim = EMBEDDING_DIM, hidden_dim = HIDDEN_DIM, \n",
    "                   vocab_size = len(word_to_ix), tag_size = len(tags_to_ix))\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "MODEL PARAMETERS : \n",
      "<bound method Module.parameters of LSTMTagger(\n",
      "  (word_embeddings): Embedding(9, 6)\n",
      "  (lstm): LSTM(6, 6)\n",
      "  (hidden2tag): Linear(in_features=6, out_features=3, bias=True)\n",
      ")>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LOSS FUNCTION : NLLLoss()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "OPTIMIZER : \n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.1\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-'* 100)\n",
    "print('MODEL PARAMETERS : \\n{}'.format(model.parameters))\n",
    "print('-'* 100)\n",
    "print('LOSS FUNCTION : {}'.format(loss_function))\n",
    "print('-'* 100)\n",
    "print('OPTIMIZER : \\n{}'.format(optimizer))\n",
    "print('-'* 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1389, -1.2024, -0.9693],\n",
      "        [-1.1065, -1.2200, -0.9834],\n",
      "        [-1.1286, -1.2093, -0.9726],\n",
      "        [-1.1190, -1.1960, -0.9916],\n",
      "        [-1.0137, -1.2642, -1.0366]])\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are before training\n",
    "# Note: The element (i,j) is the score for tag j for word i\n",
    "# Here we are wrapping the code in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_data(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is 114.30072784423828\n",
      "loss at epoch 10 is 106.49176788330078\n",
      "loss at epoch 20 is 102.92794799804688\n",
      "loss at epoch 30 is 99.87355041503906\n",
      "loss at epoch 40 is 95.85249328613281\n",
      "loss at epoch 50 is 89.87005615234375\n",
      "loss at epoch 60 is 81.1834487915039\n",
      "loss at epoch 70 is 69.76107788085938\n",
      "loss at epoch 80 is 57.036285400390625\n",
      "loss at epoch 90 is 45.24212646484375\n",
      "loss at epoch 100 is 35.56020736694336\n",
      "loss at epoch 110 is 28.075408935546875\n",
      "loss at epoch 120 is 22.45100975036621\n",
      "loss at epoch 130 is 18.260948181152344\n",
      "loss at epoch 140 is 15.123881340026855\n",
      "loss at epoch 150 is 12.744762420654297\n",
      "loss at epoch 160 is 10.910616874694824\n",
      "loss at epoch 170 is 9.471939086914062\n",
      "loss at epoch 180 is 8.324309349060059\n",
      "loss at epoch 190 is 7.394327163696289\n",
      "loss at epoch 200 is 6.629767417907715\n",
      "loss at epoch 210 is 5.992938995361328\n",
      "loss at epoch 220 is 5.456202030181885\n",
      "loss at epoch 230 is 4.9989914894104\n",
      "loss at epoch 240 is 4.605775833129883\n",
      "loss at epoch 250 is 4.264660835266113\n",
      "loss at epoch 260 is 3.966421604156494\n",
      "loss at epoch 270 is 3.7038121223449707\n",
      "loss at epoch 280 is 3.4710819721221924\n",
      "loss at epoch 290 is 3.2636172771453857\n"
     ]
    }
   ],
   "source": [
    "# Training LSTM\n",
    "\n",
    "for i in range(300):\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1: Remember that PyTorch accumulates gradients.\n",
    "        # We nee dto clear them before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Also we need to clear the history of the hidden layer.\n",
    "        # Detaching it from the hidden state of last instance\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        # Step 2: Get our input tensor readyfor the network, i.e.,\n",
    "        # turn them into vectors of word indices\n",
    "        sentence_in = prepare_data(sentence, word_to_ix)\n",
    "        targets = prepare_data(tags, tags_to_ix)\n",
    "        \n",
    "        # Steps 3: Run our forward pass \n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        # Step 4: Compute loss\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        \n",
    "        # Step 5: Compute Gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 6: Optimizer.step()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print('loss at epoch {} is {}'.format(i, loss*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0858, -2.9355, -3.5374],\n",
      "        [-5.2313, -0.0234, -4.0314],\n",
      "        [-3.9098, -4.1279, -0.0368],\n",
      "        [-0.0187, -4.7809, -4.5960],\n",
      "        [-5.8170, -0.0183, -4.1879]])\n"
     ]
    }
   ],
   "source": [
    "# Weights After Training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_data(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everybody : NN\n",
      "ate : V\n",
      "that : DET\n",
      "apple : NN\n"
     ]
    }
   ],
   "source": [
    "test_sentence = 'Everybody ate that apple'.split()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_data(test_sentence, word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    \n",
    "    tags = [ix_to_tags[torch.argmax(i).item()] for i in tag_scores]\n",
    "    \n",
    "    for word, tag in zip(test_sentence, tags):\n",
    "        print('{} : {}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DONE!!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
